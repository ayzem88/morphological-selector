import re
import ast
import os
import json
import sqlite3
import concurrent.futures
from collections import defaultdict
import multiprocessing
import logging
from datetime import datetime
import hashlib
from pathlib import Path
import pickle
from tqdm import tqdm
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¬Ù‡Ø© Ø±Ø³ÙˆÙ…ÙŠØ©
from jinja2 import Template

# Ø¯Ø¹Ù… Ù…Ù„ÙØ§Øª docx
try:
    from docx import Document
    DOCX_SUPPORT = True
except ImportError:
    DOCX_SUPPORT = False
    logging.warning("Ù…ÙƒØªØ¨Ø© python-docx ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. Ù…Ù„ÙØ§Øª .docx Ù„Ù† ØªÙÙ‚Ø±Ø£. Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØªÙ‡Ø§: pip install python-docx")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

##################################
# Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ§Ø´
##################################

class CacheManager:
    """Ù†Ø¸Ø§Ù… ÙƒØ§Ø´ Ù„Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹"""
    def __init__(self, cache_dir="cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / "word_cache.json"
        self.cache = self.load_cache()
        
    def load_cache(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ§Ø´ Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def save_cache(self):
        """Ø­ÙØ¸ Ø§Ù„ÙƒØ§Ø´ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„Ù"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
    
    def get_cache_key(self, word, pattern):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯ Ù„Ù„ÙƒÙ„Ù…Ø© ÙˆØ§Ù„ÙˆØ²Ù†"""
        return f"{word}_{pattern}"
    
    def get(self, word, pattern):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ù…Ù† Ø§Ù„ÙƒØ§Ø´"""
        key = self.get_cache_key(word, pattern)
        return self.cache.get(key)
    
    def set(self, word, pattern, result):
        """Ø­ÙØ¸ Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„ÙƒØ§Ø´"""
        key = self.get_cache_key(word, pattern)
        self.cache[key] = result
        
    def clear(self):
        """Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´"""
        self.cache = {}
        if self.cache_file.exists():
            self.cache_file.unlink()

##################################
# Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
##################################

class DatabaseManager:
    """Ù…Ø¯ÙŠØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª SQLite"""
    def __init__(self, db_path="morphology.db"):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.create_tables()
        
    def create_tables(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„"""
        cursor = self.conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern TEXT UNIQUE NOT NULL,
                pattern_type TEXT,
                frequency INTEGER DEFAULT 0,
                extra_chars_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¬Ø°ÙˆØ±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS roots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                root TEXT UNIQUE NOT NULL,
                frequency INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word TEXT NOT NULL,
                root_id INTEGER,
                pattern_id INTEGER,
                prefix TEXT,
                suffix TEXT,
                intermediate TEXT,
                frequency INTEGER DEFAULT 1,
                score REAL DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (root_id) REFERENCES roots(id),
                FOREIGN KEY (pattern_id) REFERENCES patterns(id),
                UNIQUE(word, root_id, pattern_id)
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS statistics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                run_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                total_words INTEGER,
                unique_words INTEGER,
                total_patterns INTEGER,
                total_roots INTEGER,
                success_rate REAL,
                processing_time REAL
            )
        ''')
        
        self.conn.commit()
    
    def insert_pattern(self, pattern, pattern_type=None, extra_chars_count=0):
        """Ø¥Ø¯Ø±Ø§Ø¬ ÙˆØ²Ù† Ø¬Ø¯ÙŠØ¯"""
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR IGNORE INTO patterns (pattern, pattern_type, extra_chars_count)
            VALUES (?, ?, ?)
        ''', (pattern, pattern_type, extra_chars_count))
        self.conn.commit()
        return cursor.lastrowid or self.get_pattern_id(pattern)
    
    def insert_root(self, root):
        """Ø¥Ø¯Ø±Ø§Ø¬ Ø¬Ø°Ø± Ø¬Ø¯ÙŠØ¯"""
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR IGNORE INTO roots (root)
            VALUES (?)
        ''', (root,))
        self.conn.commit()
        return cursor.lastrowid or self.get_root_id(root)
    
    def insert_result(self, word, root, pattern, prefix, suffix, intermediate, score=0):
        """Ø¥Ø¯Ø±Ø§Ø¬ Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„"""
        root_id = self.insert_root(root)
        pattern_id = self.get_pattern_id(pattern)
        
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO results 
            (word, root_id, pattern_id, prefix, suffix, intermediate, score, frequency)
            VALUES (?, ?, ?, ?, ?, ?, ?,
                COALESCE((SELECT frequency + 1 FROM results 
                         WHERE word = ? AND root_id = ? AND pattern_id = ?), 1))
        ''', (word, root_id, pattern_id, prefix, suffix, intermediate, score,
              word, root_id, pattern_id))
        
        # ØªØ­Ø¯ÙŠØ« ØªÙƒØ±Ø§Ø± Ø§Ù„ÙˆØ²Ù† ÙˆØ§Ù„Ø¬Ø°Ø±
        cursor.execute('UPDATE patterns SET frequency = frequency + 1 WHERE id = ?', (pattern_id,))
        cursor.execute('UPDATE roots SET frequency = frequency + 1 WHERE id = ?', (root_id,))
        
        self.conn.commit()
    
    def get_pattern_id(self, pattern):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±Ù Ø§Ù„ÙˆØ²Ù†"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT id FROM patterns WHERE pattern = ?', (pattern,))
        result = cursor.fetchone()
        return result[0] if result else None
    
    def get_root_id(self, root):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±Ù Ø§Ù„Ø¬Ø°Ø±"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT id FROM roots WHERE root = ?', (root,))
        result = cursor.fetchone()
        return result[0] if result else None
    
    def get_statistics(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        cursor = self.conn.cursor()
        stats = {}
        
        cursor.execute('SELECT COUNT(*) FROM results')
        stats['total_results'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT word) FROM results')
        stats['unique_words'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM patterns')
        stats['total_patterns'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM roots')
        stats['total_roots'] = cursor.fetchone()[0]
        
        cursor.execute('''
            SELECT pattern, frequency 
            FROM patterns 
            ORDER BY frequency DESC 
            LIMIT 10
        ''')
        stats['top_patterns'] = cursor.fetchall()
        
        cursor.execute('''
            SELECT root, frequency 
            FROM roots 
            ORDER BY frequency DESC 
            LIMIT 10
        ''')
        stats['top_roots'] = cursor.fetchall()
        
        return stats
    
    def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        self.conn.close()

##################################
# Ù†Ø¸Ø§Ù… ØªØ±Ø´ÙŠØ­ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£ÙˆØ²Ø§Ù†
##################################

class PatternRanker:
    """Ù†Ø¸Ø§Ù… ØªØ±Ø´ÙŠØ­ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©"""
    def __init__(self, db_manager=None):
        self.extra_chars = set("Ø³Ø£Ø¤Ø¦Ø¡Ø¢Ø¥ØªÙ…ÙˆÙ†ÙŠÙ‡Ù‰Ù‘Ø§")
        self.db_manager = db_manager
        self.pattern_scores = defaultdict(float)
        
    def calculate_score(self, pattern, word, prefix, suffix, results_count):
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ²Ù†"""
        score = 0
        
        # 1. Ù†Ù‚Ø§Ø· Ø£Ø­Ø±Ù Ø§Ù„Ø²ÙŠØ§Ø¯Ø© (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰)
        extra_count = sum(1 for c in pattern if c in self.extra_chars)
        score += extra_count * 20
        
        # 2. Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if self.db_manager:
            pattern_id = self.db_manager.get_pattern_id(pattern)
            if pattern_id:
                cursor = self.db_manager.conn.cursor()
                cursor.execute('SELECT frequency FROM patterns WHERE id = ?', (pattern_id,))
                result = cursor.fetchone()
                if result:
                    score += min(result[0] * 0.5, 50)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 50 Ù†Ù‚Ø·Ø©
        
        # 3. Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚
        if prefix:
            score += 5
        if suffix:
            score += 5
            
        # 4. Ù†Ù‚Ø§Ø· Ù†Ø³Ø¨Ø© Ø·ÙˆÙ„ Ø§Ù„ÙˆØ²Ù† Ù„Ù„ÙƒÙ„Ù…Ø©
        length_ratio = len(pattern) / len(word) if len(word) > 0 else 0
        if 0.7 <= length_ratio <= 1.3:
            score += 10
            
        # 5. Ù†Ù‚Ø§Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
        score += min(results_count * 2, 20)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 20 Ù†Ù‚Ø·Ø©
        
        return score
    
    def rank_patterns(self, patterns_results, word):
        """ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·"""
        ranked = []
        
        for pattern, results in patterns_results.items():
            total_score = 0
            for prefix, root, suffix in results:
                score = self.calculate_score(pattern, word, prefix, suffix, len(results))
                total_score += score
            
            avg_score = total_score / len(results) if results else 0
            ranked.append((pattern, results, avg_score))
        
        # ØªØ±ØªÙŠØ¨ ØªÙ†Ø§Ø²Ù„ÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
        ranked.sort(key=lambda x: x[2], reverse=True)
        
        return ranked

##################################
# Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¯ÙÙØ¹Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
##################################

class BatchProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø¯ÙÙØ¹Ø§Øª Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"""
    def __init__(self, chunk_size=1000, save_interval=5000):
        self.chunk_size = chunk_size
        self.save_interval = save_interval
        self.processed_count = 0
        self.checkpoint_file = "processing_checkpoint.pkl"
        
    def save_checkpoint(self, data):
        """Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ø³ØªØ¹Ø§Ø¯Ø©"""
        with open(self.checkpoint_file, 'wb') as f:
            pickle.dump({
                'processed_count': self.processed_count,
                'timestamp': datetime.now(),
                'data': data
            }, f)
    
    def load_checkpoint(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def process_file_in_chunks(self, file_path, process_func):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª"""
        checkpoint = self.load_checkpoint()
        start_line = 0
        
        if checkpoint:
            response = input(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù‚Ø·Ø© Ø§Ø³ØªØ¹Ø§Ø¯Ø© ({checkpoint['timestamp']}). Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ù† Ø­ÙŠØ« ØªÙˆÙ‚ÙØªØŸ (y/n): ")
            if response.lower() == 'y':
                start_line = checkpoint['processed_count']
                logging.info(f"Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù† Ø§Ù„Ø³Ø·Ø± {start_line}")
        
        results = []
        chunk = []
        
        with open(file_path, 'r', encoding='utf-8') as file:
            # ØªØ®Ø·ÙŠ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹
            for _ in range(start_line):
                next(file, None)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ù„Ù
            with tqdm(total=sum(1 for _ in open(file_path, 'r', encoding='utf-8')) - start_line,
                     desc="Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù", unit="Ø³Ø·Ø±") as pbar:
                
                for line_num, line in enumerate(file, start=start_line):
                    chunk.append(line.strip())
                    self.processed_count = line_num
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© Ø¹Ù†Ø¯ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯
                    if len(chunk) >= self.chunk_size:
                        chunk_results = process_func(chunk)
                        results.extend(chunk_results)
                        chunk = []
                        pbar.update(self.chunk_size)
                    
                    # Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ø³ØªØ¹Ø§Ø¯Ø©
                    if line_num % self.save_interval == 0:
                        self.save_checkpoint(results)
                        logging.info(f"ØªÙ… Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø³Ø·Ø± {line_num}")
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¢Ø®Ø± Ø¯ÙØ¹Ø©
                if chunk:
                    chunk_results = process_func(chunk)
                    results.extend(chunk_results)
                    pbar.update(len(chunk))
        
        # Ø­Ø°Ù Ù…Ù„Ù Ù†Ù‚Ø·Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡
        if os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)
        
        return results

##################################
# Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
##################################

class ReportGenerator:
    """Ù…ÙˆÙ„Ø¯ ØªÙ‚Ø§Ø±ÙŠØ± HTML ÙˆExcel"""
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.report_dir = Path("reports")
        self.report_dir.mkdir(exist_ok=True)
        
    def generate_text_report(self, stats, coverage=None, output_file="report.txt"):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ ÙÙ‚Ø·"""
        lines = []
        lines.append("ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ")
        lines.append("======================")
        lines.append(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {stats.get('total_results', 0):,}")
        lines.append(f"Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©: {stats.get('unique_words', 0):,}")
        lines.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {stats.get('total_patterns', 0):,}")
        lines.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ø°ÙˆØ±: {stats.get('total_roots', 0):,}")
        if 'processing_time' in stats:
            lines.append(f"ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {stats['processing_time']:.2f} Ø«Ø§Ù†ÙŠØ©")
        lines.append("")
        if coverage:
            total_words = coverage.get('total_words', 0)
            recognized = coverage.get('recognized', 0)
            unrecognized = coverage.get('unrecognized', 0)
            percent = (recognized / total_words * 100) if total_words else 0.0
            lines.append("Ù…Ù„Ø®Øµ Ø§Ù„ØªØºØ·ÙŠØ© (Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‚Ø±ÙˆØ¡Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©)")
            lines.append("-------------------------------------------")
            lines.append(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©: {total_words:,}")
            lines.append(f"Ø§Ù„Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {recognized:,} ({percent:.2f}%)")
            lines.append(f"ØºÙŠØ± Ø§Ù„Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {unrecognized:,} ({100.0 - percent:.2f}%)")
            if 'recognized_file' in coverage and 'unrecognized_file' in coverage:
                lines.append("")
                lines.append(f"Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {coverage['recognized_file']}")
                lines.append(f"Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {coverage['unrecognized_file']}")
            if 'coverage_html' in coverage:
                lines.append(f"Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ (HTML): {coverage['coverage_html']}")

        output_path = self.report_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines) + "\n")
        logging.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ: {output_path}")
        return output_path

    def generate_coverage_outputs(self, all_words_set, recognized_set,
                                   recognized_file_name="recognized_words.txt",
                                   unrecognized_file_name="unrecognized_words.txt",
                                   coverage_html_name="coverage.html"):
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØºØ·ÙŠØ©: Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆÙ…Ù„Ù HTML ØªÙØ§Ø¹Ù„ÙŠ ØµØºÙŠØ±"""
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        all_words_sorted = sorted(all_words_set)
        recognized_sorted = sorted(recognized_set & all_words_set)
        unrecognized_sorted = sorted(all_words_set - recognized_set)

        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©
        recognized_path = self.report_dir / recognized_file_name
        unrecognized_path = self.report_dir / unrecognized_file_name

        with open(recognized_path, 'w', encoding='utf-8') as f:
            for w in recognized_sorted:
                f.write(w + "\n")

        with open(unrecognized_path, 'w', encoding='utf-8') as f:
            for w in unrecognized_sorted:
                f.write(w + "\n")

        # Ø¥Ù†Ø´Ø§Ø¡ HTML ØªÙØ§Ø¹Ù„ÙŠ Ø¨Ø³ÙŠØ· (Canvas) Ø¨Ø¯ÙˆÙ† Ù…ÙƒØªØ¨Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©
        recognized_count = len(recognized_sorted)
        unrecognized_count = len(unrecognized_sorted)
        total = max(1, recognized_count + unrecognized_count)

        html_content = f"""
<!DOCTYPE html>
<html dir=\"rtl\" lang=\"ar\">
<head>
  <meta charset=\"UTF-8\" />
  <title>ØªØºØ·ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª</title>
  <style>
    body {{ font-family: Arial, Tahoma, sans-serif; background:#f7f7f7; margin:20px; }}
    .card {{ max-width: 560px; margin: 0 auto; background:#fff; padding:20px; border-radius:12px; box-shadow:0 4px 18px rgba(0,0,0,.08); }}
    h1 {{ margin-top:0; font-size:20px; color:#333; }}
    .legend {{ display:flex; gap:14px; margin:10px 0 0; align-items:center; flex-wrap:wrap; }}
    .legend-item {{ display:flex; gap:8px; align-items:center; font-size:14px; color:#444; }}
    .box {{ width:14px; height:14px; border-radius:3px; }}
    .muted {{ color:#666; font-size:13px; margin-top:6px; }}
    canvas {{ display:block; margin: 10px auto; }}
    .tooltip {{ position:absolute; background:rgba(0,0,0,.8); color:#fff; padding:6px 10px; border-radius:6px; font-size:12px; pointer-events:none; transform:translate(-50%, -140%); white-space:nowrap; }}
  </style>
</head>
<body>
  <div class=\"card\">
    <h1>Ù…Ø®Ø·Ø· Ø§Ù„ØªØºØ·ÙŠØ© (Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‚Ø±ÙˆØ¡Ø©)</h1>
    <canvas id=\"chart\" width=\"420\" height=\"420\" aria-label=\"Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ¹Ø±Ù\" role=\"img\"></canvas>
    <div class=\"legend\">
      <div class=\"legend-item\"><span class=\"box\" style=\"background:#4CAF50\"></span> Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {recognized_count} / {total}</div>
      <div class=\"legend-item\"><span class=\"box\" style=\"background:#E53935\"></span> ØºÙŠØ± Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {unrecognized_count} / {total}</div>
    </div>
    <div class=\"muted\">Ø­Ø±Ù‘Ùƒ Ø§Ù„Ù…Ø¤Ø´Ø± ÙÙˆÙ‚ Ø§Ù„Ù…Ø®Ø·Ø· Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø³Ø¨.</div>
  </div>
  <div id=\"tt\" class=\"tooltip\" style=\"display:none\"></div>
  <script>
    (function(){{
      const recognized = {recognized_count};
      const unknown = {unrecognized_count};
      const total = Math.max(1, recognized + unknown);
      const data = [recognized, unknown];
      const colors = ['#4CAF50', '#E53935'];
      const labels = ['Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§', 'ØºÙŠØ± Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§'];

      const canvas = document.getElementById('chart');
      const ctx = canvas.getContext('2d');
      const cx = canvas.width/2, cy = canvas.height/2, r = 150, ir = 90;

      function drawPie(){{
        let start = -Math.PI/2;
        ctx.clearRect(0,0,canvas.width,canvas.height);
        // Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ
        for (let i=0;i<data.length;i++){{
          const angle = (data[i]/total) * Math.PI*2;
          ctx.beginPath();
          ctx.moveTo(cx, cy);
          ctx.arc(cx, cy, r, start, start+angle);
          ctx.closePath();
          ctx.fillStyle = colors[i];
          ctx.fill();
          start += angle;
        }}
        // Ø«Ù‚Ø¨ Ø¯Ø§Ø®Ù„ÙŠ (Ø¯ÙˆÙ†Ø§Øª)
        ctx.globalCompositeOperation = 'destination-out';
        ctx.beginPath();
        ctx.arc(cx, cy, ir, 0, Math.PI*2);
        ctx.fill();
        ctx.globalCompositeOperation = 'source-over';

        // Ù†Øµ Ø§Ù„ÙˆØ³Ø·
        const percent = Math.round((recognized/total)*1000)/10;
        ctx.fillStyle = '#333';
        ctx.font = 'bold 22px Tahoma, Arial';
        ctx.textAlign = 'center';
        ctx.textBaseline = 'middle';
        ctx.fillText(percent + '%', cx, cy-6);
        ctx.font = '13px Tahoma, Arial';
        ctx.fillText('Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§', cx, cy+14);
      }}

      function hitTest(x, y){{
        const dx = x - cx, dy = y - cy; const d = Math.sqrt(dx*dx + dy*dy);
        if (d < ir || d > r) return -1;
        let angle = Math.atan2(dy, dx);
        if (angle < -Math.PI/2) angle += Math.PI*2; // Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ø¨Ø¯Ø¡
        let acc = -Math.PI/2;
        for (let i=0;i<data.length;i++){{
          const a = (data[i]/total)*Math.PI*2;
          if (angle >= acc && angle < acc + a) return i;
          acc += a;
        }}
        return -1;
      }}

      const tooltip = document.getElementById('tt');
      canvas.addEventListener('mousemove', (e)=>{{
        const rect = canvas.getBoundingClientRect();
        const x = e.clientX - rect.left; const y = e.clientY - rect.top;
        const i = hitTest(x, y);
        if (i === -1) {{ tooltip.style.display = 'none'; return; }}
        const value = data[i];
        const pct = Math.round((value/total)*1000)/10;
        tooltip.style.display = 'block';
        tooltip.textContent = labels[i] + ': ' + value + ' (' + pct + '%)';
        tooltip.style.left = (e.pageX) + 'px';
        tooltip.style.top = (e.pageY) + 'px';
      }});
      canvas.addEventListener('mouseleave', ()=>{{ tooltip.style.display='none'; }});

      drawPie();
    }})();
  </script>
  </body>
  </html>
        """

        coverage_path = self.report_dir / coverage_html_name
        with open(coverage_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        logging.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØºØ·ÙŠØ©: {recognized_path}, {unrecognized_path}, {coverage_path}")

        return {
            'total_words': len(all_words_sorted),
            'recognized': recognized_count,
            'unrecognized': unrecognized_count,
            'recognized_file': str(recognized_path),
            'unrecognized_file': str(unrecognized_path),
            'coverage_html': str(coverage_path)
        }

    def generate_html_report(self, stats, output_file="report.html"):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± HTML"""
        template_str = '''
        <!DOCTYPE html>
        <html dir="rtl" lang="ar">
        <head>
            <meta charset="UTF-8">
            <title>ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ</title>
            <style>
                body {
                    font-family: 'Arial', 'Tahoma', sans-serif;
                    margin: 20px;
                    background-color: #f5f5f5;
                }
                .container {
                    max-width: 1200px;
                    margin: 0 auto;
                    background-color: white;
                    padding: 20px;
                    border-radius: 10px;
                    box-shadow: 0 0 10px rgba(0,0,0,0.1);
                }
                h1, h2 {
                    color: #333;
                    border-bottom: 2px solid #4CAF50;
                    padding-bottom: 10px;
                }
                .stats-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 20px;
                    margin: 20px 0;
                }
                .stat-card {
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 20px;
                    border-radius: 10px;
                    text-align: center;
                }
                .stat-number {
                    font-size: 2em;
                    font-weight: bold;
                    margin: 10px 0;
                }
                table {
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                }
                th, td {
                    padding: 12px;
                    text-align: right;
                    border-bottom: 1px solid #ddd;
                }
                th {
                    background-color: #4CAF50;
                    color: white;
                }
                tr:hover {
                    background-color: #f5f5f5;
                }
                .chart-container {
                    margin: 20px 0;
                    text-align: center;
                }
                .timestamp {
                    text-align: center;
                    color: #666;
                    margin-top: 20px;
                }
                .progress-bar {
                    background-color: #f0f0f0;
                    border-radius: 10px;
                    overflow: hidden;
                    margin: 10px 0;
                }
                .progress-fill {
                    background: linear-gradient(90deg, #4CAF50, #45a049);
                    height: 30px;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    color: white;
                    font-weight: bold;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ</h1>
                
                <div class="stats-grid">
                    <div class="stat-card">
                        <div>Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬</div>
                        <div class="stat-number">{{ total_results }}</div>
                    </div>
                    <div class="stat-card">
                        <div>Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©</div>
                        <div class="stat-number">{{ unique_words }}</div>
                    </div>
                    <div class="stat-card">
                        <div>Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù†</div>
                        <div class="stat-number">{{ total_patterns }}</div>
                    </div>
                    <div class="stat-card">
                        <div>Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ø°ÙˆØ±</div>
                        <div class="stat-number">{{ total_roots }}</div>
                    </div>
                </div>
                
                <h2>ğŸ† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Ø§Ù„ØªØ±ØªÙŠØ¨</th>
                            <th>Ø§Ù„ÙˆØ²Ù†</th>
                            <th>Ø§Ù„ØªÙƒØ±Ø§Ø±</th>
                            <th>Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for i, (pattern, freq) in enumerate(top_patterns, 1) %}
                        <tr>
                            <td>{{ i }}</td>
                            <td style="font-weight: bold;">{{ pattern }}</td>
                            <td>{{ freq }}</td>
                            <td>
                                <div class="progress-bar">
                                    <div class="progress-fill" style="width: {{ (freq/max_pattern_freq)*100 }}%;">
                                        {{ "%.1f"|format((freq/total_pattern_freq)*100) }}%
                                    </div>
                                </div>
                            </td>
                        </tr>
                        {% endfor %}
                    </tbody>
                </table>
                
                <h2>ğŸŒ³ Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Ø§Ù„ØªØ±ØªÙŠØ¨</th>
                            <th>Ø§Ù„Ø¬Ø°Ø±</th>
                            <th>Ø§Ù„ØªÙƒØ±Ø§Ø±</th>
                            <th>Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for i, (root, freq) in enumerate(top_roots, 1) %}
                        <tr>
                            <td>{{ i }}</td>
                            <td style="font-weight: bold;">{{ root }}</td>
                            <td>{{ freq }}</td>
                            <td>
                                <div class="progress-bar">
                                    <div class="progress-fill" style="width: {{ (freq/max_root_freq)*100 }}%;">
                                        {{ "%.1f"|format((freq/total_root_freq)*100) }}%
                                    </div>
                                </div>
                            </td>
                        </tr>
                        {% endfor %}
                    </tbody>
                </table>
                
                <div class="chart-container">
                    <h2>ğŸ“ˆ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©</h2>
                    <img src="patterns_chart.png" alt="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†" style="max-width: 100%;">
                    <img src="roots_chart.png" alt="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬Ø°ÙˆØ±" style="max-width: 100%;">
                </div>
                
                <div class="timestamp">
                    ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {{ timestamp }}
                </div>
            </div>
        </body>
        </html>
        '''
        
        template = Template(template_str)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        max_pattern_freq = max([f for _, f in stats['top_patterns']]) if stats['top_patterns'] else 1
        total_pattern_freq = sum([f for _, f in stats['top_patterns']])
        max_root_freq = max([f for _, f in stats['top_roots']]) if stats['top_roots'] else 1
        total_root_freq = sum([f for _, f in stats['top_roots']])
        
        html_content = template.render(
            total_results=stats['total_results'],
            unique_words=stats['unique_words'],
            total_patterns=stats['total_patterns'],
            total_roots=stats['total_roots'],
            top_patterns=stats['top_patterns'],
            top_roots=stats['top_roots'],
            max_pattern_freq=max_pattern_freq,
            total_pattern_freq=total_pattern_freq,
            max_root_freq=max_root_freq,
            total_root_freq=total_root_freq,
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            enumerate=enumerate
        )
        
        output_path = self.report_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
        self.generate_charts(stats)
        
        logging.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± HTML: {output_path}")
        return output_path
    
    def generate_charts(self, stats):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©"""
        plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma']
        
        # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ø£ÙˆØ²Ø§Ù†
        if stats['top_patterns']:
            patterns, frequencies = zip(*stats['top_patterns'])
            
            plt.figure(figsize=(12, 6))
            plt.bar(range(len(patterns)), frequencies, color='#4CAF50')
            plt.xlabel('Ø§Ù„ÙˆØ²Ù†', fontsize=12)
            plt.ylabel('Ø§Ù„ØªÙƒØ±Ø§Ø±', fontsize=12)
            plt.title('Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹', fontsize=14, fontweight='bold')
            plt.xticks(range(len(patterns)), patterns, rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(self.report_dir / 'patterns_chart.png', dpi=100, bbox_inches='tight')
            plt.close()
        
        # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ø¬Ø°ÙˆØ±
        if stats['top_roots']:
            roots, frequencies = zip(*stats['top_roots'])
            
            plt.figure(figsize=(12, 6))
            plt.bar(range(len(roots)), frequencies, color='#2196F3')
            plt.xlabel('Ø§Ù„Ø¬Ø°Ø±', fontsize=12)
            plt.ylabel('Ø§Ù„ØªÙƒØ±Ø§Ø±', fontsize=12)
            plt.title('Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹', fontsize=14, fontweight='bold')
            plt.xticks(range(len(roots)), roots, rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(self.report_dir / 'roots_chart.png', dpi=100, bbox_inches='tight')
            plt.close()
    
    def generate_excel_report(self, stats, output_file="report.xlsx"):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Excel"""
        output_path = self.report_dir / output_file
        
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ÙˆØ±Ù‚Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
            summary_data = {
                'Ø§Ù„Ù…Ø¤Ø´Ø±': ['Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬', 'Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©', 'Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù†', 'Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ø°ÙˆØ±'],
                'Ø§Ù„Ù‚ÙŠÙ…Ø©': [stats['total_results'], stats['unique_words'], 
                          stats['total_patterns'], stats['total_roots']]
            }
            df_summary = pd.DataFrame(summary_data)
            df_summary.to_excel(writer, sheet_name='Ù…Ù„Ø®Øµ', index=False)
            
            # ÙˆØ±Ù‚Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
            if stats['top_patterns']:
                patterns_data = {
                    'Ø§Ù„ÙˆØ²Ù†': [p for p, _ in stats['top_patterns']],
                    'Ø§Ù„ØªÙƒØ±Ø§Ø±': [f for _, f in stats['top_patterns']]
                }
                df_patterns = pd.DataFrame(patterns_data)
                df_patterns.to_excel(writer, sheet_name='Ø§Ù„Ø£ÙˆØ²Ø§Ù†', index=False)
            
            # ÙˆØ±Ù‚Ø© Ø§Ù„Ø¬Ø°ÙˆØ±
            if stats['top_roots']:
                roots_data = {
                    'Ø§Ù„Ø¬Ø°Ø±': [r for r, _ in stats['top_roots']],
                    'Ø§Ù„ØªÙƒØ±Ø§Ø±': [f for _, f in stats['top_roots']]
                }
                df_roots = pd.DataFrame(roots_data)
                df_roots.to_excel(writer, sheet_name='Ø§Ù„Ø¬Ø°ÙˆØ±', index=False)
        
        logging.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Excel: {output_path}")
        return output_path

##################################
# Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¨Ø§Ø¯Ù„ÙŠ
##################################

class CrossValidator:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
    def __init__(self):
        self.validation_results = []
        
    def reconstruct_word(self, root, pattern, prefix="", suffix=""):
        """Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ù† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª"""
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£Ø­Ø±Ù Ø§Ù„ÙˆØ²Ù† (ÙØ¹Ù„) Ø¨Ø£Ø­Ø±Ù Ø§Ù„Ø¬Ø°Ø±
        reconstructed = pattern
        root_chars = list(root)
        pattern_chars = ['Ù', 'Ø¹', 'Ù„']
        
        for i, char in enumerate(pattern_chars[:len(root_chars)]):
            if char in reconstructed:
                reconstructed = reconstructed.replace(char, root_chars[i], 1)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚
        return prefix + reconstructed + suffix
    
    def validate_analysis(self, original_word, root, pattern, prefix, suffix):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        reconstructed = self.reconstruct_word(root, pattern, prefix, suffix)
        
        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        match_ratio = self.calculate_similarity(original_word, reconstructed)
        
        validation_result = {
            'original': original_word,
            'reconstructed': reconstructed,
            'root': root,
            'pattern': pattern,
            'prefix': prefix,
            'suffix': suffix,
            'match_ratio': match_ratio,
            'is_valid': match_ratio > 0.8  # Ø¹ØªØ¨Ø© 80% Ù„Ù„ØµØ­Ø©
        }
        
        self.validation_results.append(validation_result)
        return validation_result
    
    def calculate_similarity(self, word1, word2):
        """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† ÙƒÙ„Ù…ØªÙŠÙ†"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        word1_clean = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’]', '', word1)
        word2_clean = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’]', '', word2)
        
        if word1_clean == word2_clean:
            return 1.0
        
        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Levenshtein distance
        max_len = max(len(word1_clean), len(word2_clean))
        if max_len == 0:
            return 0.0
        
        distance = self.levenshtein_distance(word1_clean, word2_clean)
        return 1.0 - (distance / max_len)
    
    def levenshtein_distance(self, s1, s2):
        """Ø­Ø³Ø§Ø¨ Ù…Ø³Ø§ÙØ© Levenshtein"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1  # ÙƒØ§Ù† Ù†Ø§Ù‚Øµ Ø§Ù„Ø±Ù‚Ù… 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def get_validation_report(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚"""
        if not self.validation_results:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØ­Ù‚Ù‚"
        
        total = len(self.validation_results)
        valid = sum(1 for r in self.validation_results if r['is_valid'])
        invalid = total - valid
        
        report = f"""
        ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¨Ø§Ø¯Ù„ÙŠ:
        =====================
        Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø©: {total}
        Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©: {valid} ({valid/total*100:.1f}%)
        Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ØºÙŠØ± Ø§Ù„ØµØ­ÙŠØ­Ø©: {invalid} ({invalid/total*100:.1f}%)
        
        Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ØºÙŠØ± Ø§Ù„ØµØ­ÙŠØ­Ø©:
        """
        
        for result in self.validation_results[:10]:  # Ø£ÙˆÙ„ 10 Ø£Ø®Ø·Ø§Ø¡
            if not result['is_valid']:
                report += f"""
        Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {result['original']}
        Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ø¯ Ø¨Ù†Ø§Ø¤Ù‡Ø§: {result['reconstructed']}
        Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {result['match_ratio']:.1%}
        """
        
        return report
##################################
# Ù…Ø­Ø³Ù‘Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
##################################

class MemoryOptimizer:
   """Ù…Ø­Ø³Ù‘Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
   def __init__(self, max_memory_mb=500):
       self.max_memory_mb = max_memory_mb
       self.current_memory = 0
       
   def process_file_stream(self, file_path, process_func, batch_size=100):
       """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¯ÙÙ‚"""
       def file_generator():
           with open(file_path, 'r', encoding='utf-8') as f:
               batch = []
               for line in f:
                   batch.append(line.strip())
                   if len(batch) >= batch_size:
                       yield batch
                       batch = []
               if batch:
                   yield batch
       
       # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø§Øª ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰
       for batch in file_generator():
           results = process_func(batch)
           # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ­Ø°ÙÙ‡Ø§ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
           yield results
           del results
           del batch

##################################
# ÙØ¦Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
##################################

class ArabicProcessor:
   def __init__(self, optional_tashkeel=False, symbols_map=None, cache_manager=None):
       self.arabic_diacritics_pattern = re.compile("[Ù‹ÙŒÙÙÙÙÙ‘Ù’]")
       self.optional_tashkeel = optional_tashkeel
       self.arabic_symbols = symbols_map if symbols_map else {}
       self.cache_manager = cache_manager

   def add_optional_tashkeel_and_grouping(self, pattern):
       if self.optional_tashkeel:
           return re.sub(self.arabic_diacritics_pattern, lambda m: f"[{m.group()}]?", pattern)
       else:
           return re.sub(self.arabic_diacritics_pattern, lambda m: m.group(), pattern)

   def replace_symbols(self, word):
       return ''.join(self.arabic_symbols.get(letter, letter) for letter in word)


class DiacriticsHandler:
   DIACRITICS = 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù’Ù°'

   @staticmethod
   def remove_diacritics(word):
       return ''.join(c for c in word if c not in DiacriticsHandler.DIACRITICS)

   @staticmethod
   def group_letters_with_diacritics(word):
       letters_with_diacritics = []
       i = 0
       while i < len(word):
           c = word[i]
           if c not in DiacriticsHandler.DIACRITICS:
               letter = c
               i += 1
               while i < len(word) and word[i] in DiacriticsHandler.DIACRITICS:
                   letter += word[i]
                   i += 1
               letters_with_diacritics.append(letter)
           else:
               i += 1
       return letters_with_diacritics

   @staticmethod
   def normalize_quranic_text(word):
       """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ - ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„Ù‡Ù…Ø²Ø§Øª"""
       # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
       word = re.sub(r'[Ø¡Ø£Ø¥Ø¢]', 'Ø£', word)
       
       # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ù„ÙØ§Øª
       word = re.sub(r'[Ø§Ù±]', 'Ø§', word)
       
       # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ø§Ù„Ø®Ø§ØµØ©
       word = re.sub(r'[Ù°Ù±Ù²Ù³Ù´ÙµÙ¶Ù·Ù¸Ù¹ÙºÙ»Ù¼Ù½Ù¾Ù¿Û–Û—Û˜Û™ÛšÛ›ÛœÛÛÛŸÛ Û¡Û¢Û£Û¤Û¥Û¦Û§Û¨Û©ÛªÛ«Û¬Û­Û®Û¯]', '', word)
       
       # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ§Ø¡Ø§Øª
       word = re.sub(r'Ø©', 'Øª', word)
       
       return word


class WordSplitter:
   ROOT_INDICATORS = 'ÙØ¹Ù„'

   def __init__(self, diacritics_handler):
       self.diacritics_handler = diacritics_handler

   def split_word(self, template_word, target_word):
       template_clean = self.diacritics_handler.remove_diacritics(template_word)
       template_letters = list(template_clean)
       target_letters = self.diacritics_handler.group_letters_with_diacritics(target_word)

       if len(template_letters) != len(target_letters):
           return '', '', target_word, ''

       root_positions = [idx for idx, c in enumerate(template_letters) if c in WordSplitter.ROOT_INDICATORS]
       if not root_positions:
           return '', '', target_word, ''

       prefix_letters = []
       root_letters = []
       intermediate_letters = []
       suffix_letters = []

       first_root_pos = root_positions[0]
       last_root_pos = root_positions[-1]

       for idx, c in enumerate(template_letters):
           target_char = target_letters[idx]
           if c in WordSplitter.ROOT_INDICATORS:
               root_letters.append(target_char)
           else:
               if idx < first_root_pos:
                   prefix_letters.append(target_char)
               elif idx > last_root_pos:
                   suffix_letters.append(target_char)
               else:
                   intermediate_letters.append(target_char)

       prefix = ''.join(prefix_letters)
       intermediate = ''.join(intermediate_letters)
       root = ''.join(root_letters)
       suffix = ''.join(suffix_letters)

       return prefix, intermediate, root, suffix

##################################
# Ø¯Ø§Ù„Ø© Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
##################################

def detect_file_type(file_path, sample_lines=50):
    """
    Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹: 'list' Ø£Ùˆ 'text'
    
    Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©:
    - Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ø³Ø·Ø± > 2.0 â†’ 'text'
    - Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ø³Ø·Ø± <= 2.0 â†’ 'list'
    - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£ÙƒØ«Ø± Ù…Ù† 80% Ù…Ù† Ø§Ù„Ø£Ø³Ø·Ø± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· â†’ 'list'
    - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£ÙƒØ«Ø± Ù…Ù† 50% Ù…Ù† Ø§Ù„Ø£Ø³Ø·Ø± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 3 ÙƒÙ„Ù…Ø§Øª â†’ 'text'
    """
    arabic_word_re = re.compile(r"[\u0621-\u064A\u064B-\u065F\u0670\u06D6-\u06DC\u06DF-\u06E8\u06EA-\u06ED]+")
    word_counts = []
    single_word_lines = 0
    multi_word_lines = 0
    
    file_ext = os.path.splitext(file_path)[1].lower()
    lines = []
    
    try:
        if file_ext == '.docx':
            # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù docx
            if not DOCX_SUPPORT:
                logging.warning(f"Ù…ÙƒØªØ¨Ø© python-docx ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. Ø§Ø³ØªØ®Ø¯Ø§Ù… 'text' ÙƒØ§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù„Ù…Ù„Ù: {file_path}")
                return 'text'
            
            try:
                doc = Document(file_path)
                for i, paragraph in enumerate(doc.paragraphs):
                    if i >= sample_lines:
                        break
                    text = paragraph.text.strip()
                    if text:
                        lines.append(text)
            except Exception as e:
                logging.warning(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù docx {file_path}: {e}. Ø§Ø³ØªØ®Ø¯Ø§Ù… 'text' ÙƒØ§ÙØªØ±Ø§Ø¶ÙŠ.")
                return 'text'
        else:
            # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù txt
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= sample_lines:
                            break
                        line = line.strip()
                        if line:
                            lines.append(line)
            except Exception as e:
                logging.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù {file_path}: {e}. Ø§Ø³ØªØ®Ø¯Ø§Ù… 'text' ÙƒØ§ÙØªØ±Ø§Ø¶ÙŠ.")
                return 'text'  # Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø·Ø±
        for line in lines:
            words = arabic_word_re.findall(line)
            word_count = len(words)
            if word_count > 0:
                word_counts.append(word_count)
                if word_count == 1:
                    single_word_lines += 1
                elif word_count > 3:
                    multi_word_lines += 1
    except Exception as e:
        logging.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù {file_path}: {e}. Ø§Ø³ØªØ®Ø¯Ø§Ù… 'text' ÙƒØ§ÙØªØ±Ø§Ø¶ÙŠ.")
        return 'text'  # Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
    
    if not word_counts:
        logging.warning(f"Ø§Ù„Ù…Ù„Ù {file_path} ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø¹Ø±Ø¨ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ø§Ù… 'text' ÙƒØ§ÙØªØ±Ø§Ø¶ÙŠ.")
        return 'text'  # Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
    
    total_lines = len(word_counts)
    avg_words = sum(word_counts) / total_lines
    single_word_ratio = single_word_lines / total_lines if total_lines > 0 else 0
    multi_word_ratio = multi_word_lines / total_lines if total_lines > 0 else 0
    
    # Ù…Ù†Ø·Ù‚ Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø§ÙƒØªØ´Ø§Ù
    if single_word_ratio > 0.8:
        # Ø£ÙƒØ«Ø± Ù…Ù† 80% Ù…Ù† Ø§Ù„Ø£Ø³Ø·Ø± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© â†’ Ù‚Ø§Ø¦Ù…Ø©
        detected_type = 'list'
    elif multi_word_ratio > 0.5:
        # Ø£ÙƒØ«Ø± Ù…Ù† 50% Ù…Ù† Ø§Ù„Ø£Ø³Ø·Ø± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 3 ÙƒÙ„Ù…Ø§Øª â†’ Ù†Øµ
        detected_type = 'text'
    elif avg_words > 2.0:
        # Ù…ØªÙˆØ³Ø· Ø£ÙƒØ«Ø± Ù…Ù† 2 ÙƒÙ„Ù…Ø© â†’ Ù†Øµ
        detected_type = 'text'
    else:
        # Ù…ØªÙˆØ³Ø· Ø£Ù‚Ù„ Ù…Ù† Ø£Ùˆ ÙŠØ³Ø§ÙˆÙŠ 2 ÙƒÙ„Ù…Ø© â†’ Ù‚Ø§Ø¦Ù…Ø©
        detected_type = 'list'
    
    logging.debug(f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù {file_path}: {detected_type} (Ù…ØªÙˆØ³Ø· Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {avg_words:.2f}, "
                  f"Ù†Ø³Ø¨Ø© Ø§Ù„Ø³Ø·ÙˆØ± Ø¨ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©: {single_word_ratio:.2%}, "
                  f"Ù†Ø³Ø¨Ø© Ø§Ù„Ø³Ø·ÙˆØ± Ø¨Ø£ÙƒØ«Ø± Ù…Ù† 3 ÙƒÙ„Ù…Ø§Øª: {multi_word_ratio:.2%})")
    return detected_type

##################################
# ÙØ¦Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
##################################

class FileManager:
   EXTRA_CHARS = "Ø³Ø£Ø¤Ø¦Ø¡Ø¢Ø¥ØªÙ…ÙˆÙ†ÙŠÙ‡Ù‰Ù‘Ø§"

   def __init__(self, corpus_type='text', match_whole_word=True, affixes_data=None, 
                tags_map=None, db_manager=None, cache_manager=None, 
                pattern_ranker=None, cross_validator=None):
       self.corpus_type = corpus_type
       self.match_whole_word = match_whole_word
       affixes_data = affixes_data if affixes_data else {'prefixes': [], 'suffixes': []}

       self.prefixes = affixes_data.get('prefixes', [])
       self.suffixes = affixes_data.get('suffixes', [])

       self.prefix_pattern = f"(?:{'|'.join(map(re.escape, self.prefixes))})" if self.prefixes else ""
       self.suffix_pattern = f"(?:{'|'.join(map(re.escape, self.suffixes))})" if self.suffixes else ""

       # Ø¨Ù†Ø§Ø¡ word_boundary Ø­Ø³Ø¨ corpus_type (ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ)
       if self.corpus_type == 'list':
           if self.match_whole_word:
               self.word_boundary_start = r"^"
               self.word_boundary_end = r"$"
           else:
               self.word_boundary_start = ''
               self.word_boundary_end = ''
       elif self.corpus_type == 'text':
           if self.match_whole_word:
               self.word_boundary = r"(?:\s|^|$|[ØŒØ›.!ØŸ\"'Â«Â»()\-])"
               # Ø¥Ø¶Ø§ÙØ© Ø­Ø¯ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ù†Øµ
               self.word_boundary_start = r"(?:\s|^|[ØŒØ›.!ØŸ\"'Â«Â»()\-])"
               self.word_boundary_end = r"(?:\s|$|[ØŒØ›.!ØŸ\"'Â«Â»()\-])"
           else:
               self.word_boundary = ''
               self.word_boundary_start = ''
               self.word_boundary_end = ''
       else:
           self.word_boundary = ''

       self.diacritics_handler = DiacriticsHandler()
       self.word_splitter = WordSplitter(self.diacritics_handler)
       self.tags_map = tags_map if tags_map else {}
       self.db_manager = db_manager
       self.cache_manager = cache_manager
       self.pattern_ranker = pattern_ranker
       self.cross_validator = cross_validator

   def search_patterns_in_file(self, file_path, pattern, weight):
       """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø´ ÙˆØ§Ù„ØªØ·Ø¨ÙŠØ¹"""
       # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø£ÙˆÙ„Ø§Ù‹
       if self.cache_manager:
           cache_key = f"{file_path}_{pattern}"
           cached_result = self.cache_manager.get(cache_key, pattern)
           if cached_result:
               logging.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ù„Ù„Ù†Ù…Ø·: {pattern}")
               return cached_result
       
       # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø­Ø³Ø¨ corpus_type (ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ)
       results = []
       if self.corpus_type == 'list':
           full_pattern = f"{self.word_boundary_start}(?P<prefix>{self.prefix_pattern})?(?P<root>{pattern})(?P<suffix>{self.suffix_pattern})?{self.word_boundary_end}"
       elif self.corpus_type == 'text':
           if self.match_whole_word:
               # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø­Ø¯ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ù†Øµ
               full_pattern = f"{self.word_boundary_start}(?P<prefix>{self.prefix_pattern})?(?P<root>{pattern})(?P<suffix>{self.suffix_pattern})?{self.word_boundary_end}"
           else:
               full_pattern = f"{self.word_boundary}(?P<prefix>{self.prefix_pattern})?(?P<root>{pattern})(?P<suffix>{self.suffix_pattern})?{self.word_boundary}"
       else:
           full_pattern = f"(?P<prefix>{self.prefix_pattern})?(?P<root>{pattern})(?P<suffix>{self.suffix_pattern})?"

       compiled_pattern = re.compile(full_pattern)
       logging.debug(f"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø·: {compiled_pattern.pattern}")

       # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø­Ø³Ø¨ Ù†ÙˆØ¹Ù‡
       file_ext = os.path.splitext(file_path)[1].lower()
       
       if file_ext == '.docx':
           # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù docx
           if not DOCX_SUPPORT:
               logging.warning(f"Ù…ÙƒØªØ¨Ø© python-docx ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù„Ù: {file_path}")
               return results
           
           try:
               doc = Document(file_path)
               lines = []
               for paragraph in doc.paragraphs:
                   if paragraph.text.strip():
                       lines.append(paragraph.text)
           except Exception as e:
               logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù docx {file_path}: {e}")
               return results
           
           # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­ØªÙˆÙ‰ docx
           for line in lines:
               # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø³Ø·Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø­Ø«
               normalized_line = self.diacritics_handler.normalize_quranic_text(line)
               
               for match in compiled_pattern.finditer(normalized_line):
                   prefix = match.group('prefix') or ''
                   root = match.group('root')
                   suffix = match.group('suffix') or ''
                   
                   # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¨Ø§Ø¯Ù„ÙŠ
                   if self.cross_validator:
                       word = prefix + root + suffix
                       validation = self.cross_validator.validate_analysis(
                           word, root, pattern, prefix, suffix
                       )
                       if validation['is_valid']:
                           results.append((prefix, root, suffix))
                   else:
                       results.append((prefix, root, suffix))
       else:
           # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù txt Ø³Ø·Ø±Ø§Ù‹ Ø¨Ø³Ø·Ø± (ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ)
           with open(file_path, 'r', encoding='utf-8') as file:
               for line in file:
                   # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø³Ø·Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø­Ø«
                   normalized_line = self.diacritics_handler.normalize_quranic_text(line)
                   
                   for match in compiled_pattern.finditer(normalized_line):
                       prefix = match.group('prefix') or ''
                       root = match.group('root')
                       suffix = match.group('suffix') or ''
                       
                       # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¨Ø§Ø¯Ù„ÙŠ
                       if self.cross_validator:
                           word = prefix + root + suffix
                           validation = self.cross_validator.validate_analysis(
                               word, root, pattern, prefix, suffix
                           )
                           if validation['is_valid']:
                               results.append((prefix, root, suffix))
                       else:
                           results.append((prefix, root, suffix))
       
       # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
       if self.cache_manager and results:
           cache_key = f"{file_path}_{pattern}"
           self.cache_manager.set(cache_key, pattern, results)
       
       return results

   def _count_results(self, results):
       counts = defaultdict(int)
       for prefix, root, suffix in results:
           matched_word = prefix + root + suffix
           counts[(matched_word, prefix, root, suffix)] += 1
       return counts

   def write_results(self, folder_path, weight, results):
       """ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
       if not results:
           logging.info(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ÙˆØ²Ù†: {weight}. Ù„Ù† ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù.")
           return

       if not os.path.exists(folder_path):
           os.makedirs(folder_path, exist_ok=True)

       # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø£Ø­Ø±Ù Ø§Ù„Ø²ÙŠØ§Ø¯Ø© Ù„Ù„ÙˆØ²Ù†
       extra_chars_count = sum(1 for c in weight if c in FileManager.EXTRA_CHARS)
       
       # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„ÙˆØ²Ù† ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
       if self.db_manager:
           pattern_type = 'Ø§Ø³Ù…' if 'Ø§Ù„Ø£Ø³Ù…Ø§Ø¡' in folder_path else 'ÙØ¹Ù„'
           self.db_manager.insert_pattern(weight, pattern_type, extra_chars_count)

       file_path = os.path.join(folder_path, f"{weight}.txt")
       with open(file_path, 'w', encoding='utf-8') as file:
           for (matched_word, prefix, root, suffix), count in self._count_results(results).items():
               original_word = matched_word
               template_word = weight
               target_word = root

               prefix_morph, intermediate_morph, root_morph, suffix_morph = self.word_splitter.split_word(
                   template_word, target_word
               )
               root_without_diacritics = self.diacritics_handler.remove_diacritics(root_morph)

               prefix_output = f"[{prefix if prefix else '#'}]"
               suffix_output = f"[{suffix if suffix else '#'}]"
               intermediate_output = f"[{intermediate_morph if intermediate_morph else '#'}]"

               result_line = (f"{root_without_diacritics} | {target_word} | {template_word} | {original_word} | "
                              f"{prefix_output} | {intermediate_output} | {suffix_output} | ØªÙƒØ±Ø§Ø±: {count}")

               # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ³Ù… Ø¥Ù† ÙˆØ¬Ø¯Øª
               if template_word in self.tags_map:
                   result_line += f" | [Ø§Ù„ÙˆØ³Ù…: {self.tags_map[template_word]}]"

               file.write(f"{result_line}\n")
               
               # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
               if self.db_manager:
                   # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ù„Ù„Ù†ØªÙŠØ¬Ø©
                   score = 0
                   if self.pattern_ranker:
                       score = self.pattern_ranker.calculate_score(
                           weight, matched_word, prefix, suffix, count
                       )
                   
                   self.db_manager.insert_result(
                       matched_word, root_without_diacritics, weight,
                       prefix, suffix, intermediate_morph, score
                   )

   def read_weights_and_derived_words(self, file_path):
       weights = {}
       with open(file_path, 'r', encoding='utf-8') as file:
           for line in file:
               if not line.startswith('#'):
                   parts = line.strip().split(':')
                   if len(parts) == 2:
                       key, values = parts[0].strip(), parts[1].strip()
                       derived_words = [value.strip() for value in values.split('ØŒ')]
                       weights[key] = derived_words
                   else:
                       if parts:
                           weights[parts[0].strip()] = []

       # Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø£Ø­Ø±Ù Ø§Ù„Ø²ÙŠØ§Ø¯Ø©
       weights = self._reorder_weights(weights)
       return weights

   def _reorder_weights(self, weights_dict):
       # Ø£Ø­Ø±Ù Ø§Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
       extra_chars = set(FileManager.EXTRA_CHARS)

       def count_extra_chars(word):
           # ÙŠØ­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙÙŠ Ø§Ù„ÙˆØ²Ù†
           return sum(letter in extra_chars for letter in word)

       # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¯ÙƒØª Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† tuples: (weight, derived_words)
       weights_list = [(w, d) for w, d in weights_dict.items()]

       # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø£Ø­Ø±Ù Ø§Ù„Ø²ÙŠØ§Ø¯Ø© ØªÙ†Ø§Ø²Ù„ÙŠØ§Ù‹
       weights_list.sort(key=lambda x: count_extra_chars(x[0]), reverse=True)

       # Ø¥Ø¹Ø§Ø¯Ø© ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ dict Ø¨Ù†ÙØ³ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯
       new_weights_dict = {}
       for w, d in weights_list:
           new_weights_dict[w] = d
       return new_weights_dict

##################################
# ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
##################################

def process_weight(args):
   weight, derived_weights, file_paths, results_dir_name, corpus_type, match_whole_word, affixes_data, tags_map, symbols_map, optional_tashkeel, use_cross_validation = args
   logging.info(f"Ø¨Ø¯Ø£ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ²Ù†: {weight}")
   
   # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ©
   processor = ArabicProcessor(
       optional_tashkeel=optional_tashkeel, 
       symbols_map=symbols_map,
       cache_manager=None  # Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒØ§Ø´ ÙÙŠ multiprocessing
   )
   
   # ÙÙŠ multiprocessingØŒ Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… cross_validator (ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ)
   # cross_validator ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙ‚Ø· ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø¨Ø¹Ø¯ Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
   file_manager = FileManager(
       corpus_type=corpus_type, 
       match_whole_word=match_whole_word, 
       affixes_data=affixes_data, 
       tags_map=tags_map,
       db_manager=None,  # Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ multiprocessing
       cache_manager=None,
       pattern_ranker=None,
       cross_validator=None  # Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… cross_validator ÙÙŠ multiprocessing (ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ)
   )
   
   # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ÙˆØ²Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
   diacritics_handler = DiacriticsHandler()
   normalized_weight = diacritics_handler.normalize_quranic_text(weight)
   
   pattern = processor.add_optional_tashkeel_and_grouping(weight)
   pattern = processor.replace_symbols(pattern)

   all_results = []
   patterns_results = defaultdict(list)  # Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„ÙˆØ²Ù†
   
   for file_path in file_paths:
       logging.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {file_path} Ù„Ù„ÙˆØ²Ù†: {weight}")
       found_results = file_manager.search_patterns_in_file(file_path, pattern, weight)
       all_results.extend(found_results)
       patterns_results[weight].extend(found_results)

   folder_path = os.path.join(results_dir_name, weight)
   file_manager.write_results(folder_path, weight, all_results)

   # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø´ØªÙ‚Ø©
   for derived_weight in derived_weights:
       logging.info(f"Ø¨Ø¯Ø£ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù…Ø´ØªÙ‚: {derived_weight} Ù„Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {weight}")
       derived_pattern = processor.add_optional_tashkeel_and_grouping(derived_weight)
       derived_pattern = processor.replace_symbols(derived_pattern)

       all_derived_results = []
       for file_path in file_paths:
           logging.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {file_path} Ù„Ù„ÙˆØ²Ù† Ø§Ù„Ù…Ø´ØªÙ‚: {derived_weight}")
           found_results = file_manager.search_patterns_in_file(file_path, derived_pattern, derived_weight)
           all_derived_results.extend(found_results)
           patterns_results[derived_weight].extend(found_results)

       file_manager.write_results(folder_path, derived_weight, all_derived_results)

   logging.info(f"Ø§Ù†ØªÙ‡Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ²Ù†: {weight}")
   # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
   return {
       'weight': weight,
       'results': all_results,
       'patterns_results': dict(patterns_results),
       'count': len(all_results),
       'validation_results': []  # Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… cross_validator ÙÙŠ multiprocessing
   }

##################################
# ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©
##################################

def collect_corpus_words(file_paths, corpus_type='list'):
    """ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© (Ù…Ø¹ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„ØªØ·Ø¨ÙŠØ¹)."""
    diacritics_handler = DiacriticsHandler()
    all_words = set()
    # Ù†Ù…Ø· Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (ÙŠØ´Ù…Ù„ Ø§Ù„ØªØ´ÙƒÙŠÙ„)
    arabic_word_re = re.compile(r"[\u0621-\u064A\u064B-\u065F\u0670\u06D6-\u06DC\u06DF-\u06E8\u06EA-\u06ED]+")

    for fp in file_paths:
        try:
            # Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
            file_type = detect_file_type(fp)
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø­Ø³Ø¨ Ù†ÙˆØ¹Ù‡
            file_ext = os.path.splitext(fp)[1].lower()
            lines = []
            
            if file_ext == '.docx':
                # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù docx
                if not DOCX_SUPPORT:
                    logging.warning(f"Ù…ÙƒØªØ¨Ø© python-docx ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù„Ù: {fp}")
                    continue
                
                try:
                    doc = Document(fp)
                    for paragraph in doc.paragraphs:
                        if paragraph.text.strip():
                            lines.append(paragraph.text)
                except Exception as e:
                    logging.warning(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù docx {fp}: {e}")
                    continue
            else:
                # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù txt
                try:
                    with open(fp, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                except Exception as e:
                    logging.warning(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {fp}: {e}")
                    continue
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø·Ø±
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                if file_type == 'list':
                    word = diacritics_handler.remove_diacritics(line)
                    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ
                    word = diacritics_handler.normalize_quranic_text(word)
                    all_words.add(word)
                else:  # text
                    for m in arabic_word_re.findall(line):
                        word = diacritics_handler.remove_diacritics(m)
                        if word:
                            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ
                            word = diacritics_handler.normalize_quranic_text(word)
                            all_words.add(word)
        except Exception as e:
            logging.warning(f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªØ¬Ù…ÙŠØ¹: {fp} - {e}")
    return all_words

##################################
# Ø¯Ø§Ù„Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ³Ù…
##################################
def load_tags(tags_file_path):
   tags_map = {}
   if os.path.exists(tags_file_path):
       with open(tags_file_path, 'r', encoding='utf-8') as f:
           pattern = re.compile(r'"([^"]+)"\s*=\s*"([^"]+)"')
           for line in f:
               line = line.strip()
               match = pattern.match(line)
               if match:
                   word = match.group(1)
                   tag = match.group(2)
                   tags_map[word] = tag
   return tags_map

##################################
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
##################################

def main():
   start_time = datetime.now()
   
####################################################################################################################################################################################################################################
   optional_tashkeel = False #True or False
   #optional_tashkeel = True #True or False
   match_whole_word = True #True or False
   #match_whole_word = False #True or False
   corpus_type = 'list' # list Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© | text Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù†Øµ
   # ØªÙ… Ø¥Ø²Ø§Ù„Ø© Ø®ÙŠØ§Ø± part_of_speech - Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚Ø±Ø£ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø£ÙØ¹Ø§Ù„ Ù…Ø¹Ø§Ù‹
   use_cache = True  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ§Ø´
   use_database = True  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
   generate_report = True  # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± (Ø³ÙŠÙƒÙˆÙ† Ù†ØµÙŠÙ‹Ø§ ÙÙ‚Ø·)
   use_cross_validation = True  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¨Ø§Ø¯Ù„ÙŠ
   
   database_folder = r"Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
   symbols_file_path = os.path.join(database_folder, "Ø§Ù„Ø®Ø±ÙŠØ·Ø©.txt")
   corpus_folder = os.path.join(database_folder, "Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©")
   tags_file_path = os.path.join(database_folder, "0.3 Ø§Ù„ÙˆØ³Ù….txt")

   # Ù‚Ø±Ø§Ø¡Ø© Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø£ÙØ¹Ø§Ù„ Ù…Ø¹Ø§Ù‹
   names_results_dir = os.path.join(database_folder, "Ø§Ù„Ù†ØªØ§Ø¦Ø¬_Ø§Ù„Ø£Ø³Ù…Ø§Ø¡")
   verbs_results_dir = os.path.join(database_folder, "Ø§Ù„Ù†ØªØ§Ø¦Ø¬_Ø§Ù„Ø£ÙØ¹Ø§Ù„")
   
   names_weights_file = os.path.join(database_folder, "0.3 Ø£ÙˆØ²Ø§Ù†_Ø§Ù„Ø£Ø³Ù…Ø§Ø¡.txt")
   verbs_weights_file = os.path.join(database_folder, "0.3 Ø£ÙˆØ²Ø§Ù†_Ø§Ù„Ø£ÙØ¹Ø§Ù„.txt")
   
   names_affixes_file = os.path.join(database_folder, "0.3 Ø³ÙˆØ§Ø¨Ù‚ ÙˆÙ„ÙˆØ§Ø­Ù‚_Ø£Ø³Ù…Ø§Ø¡.txt")
   verbs_affixes_file = os.path.join(database_folder, "0.3 Ø³ÙˆØ§Ø¨Ù‚ ÙˆÙ„ÙˆØ§Ø­Ù‚_Ø£ÙØ¹Ø§Ù„.txt")

   # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±Ù…ÙˆØ²
   with open(symbols_file_path, "r", encoding="utf-8") as sf:
       symbols_map = ast.literal_eval(sf.read())

   # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡
   if os.path.exists(names_affixes_file):
       with open(names_affixes_file, 'r', encoding='utf-8') as f:
           names_affixes_data = ast.literal_eval(f.read())
   else:
       logging.warning("Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ± Ù…Ù„Ù Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø£Ùˆ Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± ØµØ­ÙŠØ­. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ø¦Ù… ÙØ§Ø±ØºØ©.")
       names_affixes_data = {'prefixes': [], 'suffixes': []}
   
   # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚ Ù„Ù„Ø£ÙØ¹Ø§Ù„
   if os.path.exists(verbs_affixes_file):
       with open(verbs_affixes_file, 'r', encoding='utf-8') as f:
           verbs_affixes_data = ast.literal_eval(f.read())
   else:
       logging.warning("Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ± Ù…Ù„Ù Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚ Ù„Ù„Ø£ÙØ¹Ø§Ù„ Ø£Ùˆ Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± ØµØ­ÙŠØ­. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ø¦Ù… ÙØ§Ø±ØºØ©.")
       verbs_affixes_data = {'prefixes': [], 'suffixes': []}

   # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ³Ù…
   tags_map = load_tags(tags_file_path)

   # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
   cache_manager = CacheManager() if use_cache else None
   db_manager = DatabaseManager() if use_database else None
   pattern_ranker = PatternRanker(db_manager) if use_database else None
   cross_validator = CrossValidator() if use_cross_validation else None
   batch_processor = BatchProcessor()
   memory_optimizer = MemoryOptimizer()
   
   # Ø¥Ù†Ø´Ø§Ø¡ FileManager Ù…Ø¤Ù‚Øª Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
   temp_file_manager = FileManager(
        corpus_type=corpus_type, 
        match_whole_word=match_whole_word, 
        affixes_data=names_affixes_data,  # Ø³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡ Ù„ÙƒÙ„ Ù†ÙˆØ¹
        tags_map=tags_map,
        db_manager=None,
        cache_manager=None,
        pattern_ranker=None,
        cross_validator=None
    )
    
   # ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
   names_weights = temp_file_manager.read_weights_and_derived_words(names_weights_file)
   
   # ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ÙØ¹Ø§Ù„
   temp_file_manager.affixes_data = verbs_affixes_data  # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚ Ù„Ù„Ø£ÙØ¹Ø§Ù„
   verbs_weights = temp_file_manager.read_weights_and_derived_words(verbs_weights_file)
   
   # Ø¯Ù…Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
   all_weights = {**names_weights, **verbs_weights}

   file_paths = [os.path.join(corpus_folder, file_name) 
                 for file_name in os.listdir(corpus_folder) 
                 if file_name.endswith('.txt')]

   # Ø¬Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºØ·ÙŠØ© Ù„Ø§Ø­Ù‚Ù‹Ø§
   all_corpus_words = collect_corpus_words(file_paths, corpus_type=corpus_type)

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
   print(f"\n{'='*60}")
   print(f"Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(all_weights)} ÙˆØ²Ù† ØµØ±ÙÙŠ (Ø£Ø³Ù…Ø§Ø¡ ÙˆØ£ÙØ¹Ø§Ù„)...")
   print(f"{'='*60}\n")
   
   # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… Ù„Ù„Ø£Ø³Ù…Ø§Ø¡
   names_tasks = [(weight, derived_weights, file_paths, names_results_dir, corpus_type, match_whole_word, names_affixes_data, tags_map, symbols_map, optional_tashkeel, use_cross_validation) 
                  for weight, derived_weights in names_weights.items()]
   
   # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… Ù„Ù„Ø£ÙØ¹Ø§Ù„
   verbs_tasks = [(weight, derived_weights, file_paths, verbs_results_dir, corpus_type, match_whole_word, verbs_affixes_data, tags_map, symbols_map, optional_tashkeel, use_cross_validation) 
                  for weight, derived_weights in verbs_weights.items()]
   
   # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù‡Ø§Ù…
   all_tasks = names_tasks + verbs_tasks

   # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
   all_processing_results = []
   with concurrent.futures.ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
       all_processing_results = list(tqdm(
           executor.map(process_weight, all_tasks),
           total=len(all_tasks),
           desc="Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†",
           unit="ÙˆØ²Ù†"
       ))
   
   # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
   if db_manager:
       print(f"\n{'='*60}")
       print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
       print(f"{'='*60}\n")
       
       diacritics_handler = DiacriticsHandler()
       word_splitter = WordSplitter(diacritics_handler)
       
       for result_data in tqdm(all_processing_results, desc="Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"):
           weight = result_data['weight']
           results = result_data['results']
           
           # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø£Ø­Ø±Ù Ø§Ù„Ø²ÙŠØ§Ø¯Ø© Ù„Ù„ÙˆØ²Ù†
           extra_chars_count = sum(1 for c in weight if c in FileManager.EXTRA_CHARS)
           
           # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙˆØ²Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø£Ùˆ Ø§Ù„Ø£ÙØ¹Ø§Ù„
           if weight in names_weights:
               pattern_type = 'Ø§Ø³Ù…'
           elif weight in verbs_weights:
               pattern_type = 'ÙØ¹Ù„'
           else:
               pattern_type = 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'  # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ ØªØ¯Ø§Ø®Ù„
           
           # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„ÙˆØ²Ù† ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
           db_manager.insert_pattern(weight, pattern_type, extra_chars_count)
           
           # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
           for prefix, root, suffix in results:
               matched_word = prefix + root + suffix
               
               # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©
               prefix_morph, intermediate_morph, root_morph, suffix_morph = word_splitter.split_word(
                   weight, root
               )
               root_without_diacritics = diacritics_handler.remove_diacritics(root_morph)
               
               # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
               score = 0
               if pattern_ranker:
                   score = pattern_ranker.calculate_score(
                       weight, matched_word, prefix, suffix, 1
                   )
               
               # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
               db_manager.insert_result(
                   matched_word, root_without_diacritics, weight,
                   prefix, suffix, intermediate_morph, score
               )

   # Ø­ÙØ¸ Ø§Ù„ÙƒØ§Ø´
   if cache_manager:
       cache_manager.save_cache()
       logging.info("ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙƒØ§Ø´")

   # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± (Ù†ØµÙŠ + Ù…Ù„ÙØ§Øª Ø§Ù„ØªØºØ·ÙŠØ©) ÙˆØ¥Ù„ØºØ§Ø¡ HTML/Excel Ø§Ù„Ù‚Ø¯ÙŠÙ…ÙŠÙ†
   if generate_report and db_manager:
       report_generator = ReportGenerator(db_manager)
       stats = db_manager.get_statistics()

       # Ø­Ø³Ø§Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
       diacritics_handler = DiacriticsHandler()
       recognized_words = set()
       for result_data in all_processing_results:
           for prefix, root, suffix in result_data['results']:
               matched_word = prefix + root + suffix
               matched_word = diacritics_handler.remove_diacritics(matched_word)
               if matched_word:
                   recognized_words.add(matched_word)

       coverage_info = report_generator.generate_coverage_outputs(
           all_words_set=all_corpus_words,
           recognized_set=recognized_words
       )

       # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆÙ‚Øª
       processing_time = (datetime.now() - start_time).total_seconds()
       stats['processing_time'] = processing_time

       txt_report = report_generator.generate_text_report(stats, coverage=coverage_info, output_file="report.txt")

       print(f"\n{'='*60}")
       print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
       print(f"{'='*60}")
       print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {stats['total_results']:,}")
       print(f"Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©: {stats['unique_words']:,}")
       print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {stats['total_patterns']:,}")
       print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ø°ÙˆØ±: {stats['total_roots']:,}")
       print(f"ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
       print(f"\nâœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±:")
       print(f"   - Ù†ØµÙŠ: {txt_report}")
       print(f"   - Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {coverage_info['recognized_file']}")
       print(f"   - Ù‚Ø§Ø¦Ù…Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ¹Ø±Ù‘Ù Ø¹Ù„ÙŠÙ‡Ø§: {coverage_info['unrecognized_file']}")
       print(f"   - Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ: {coverage_info['coverage_html']}")
   
   # Ø·Ø¨Ø§Ø¹Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¨Ø§Ø¯Ù„ÙŠ
   if cross_validator:
       print(f"\n{'='*60}")
       print("ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¨Ø§Ø¯Ù„ÙŠ:")
       print(f"{'='*60}")
       print(cross_validator.get_validation_report())
   
   # Ø¥ØºÙ„Ø§Ù‚ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
   if db_manager:
       db_manager.close()
   
   print(f"\n{'='*60}")
   print(f"âœ… Ø§Ù†ØªÙ‡Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­!")
   print(f"{'='*60}\n")

if __name__ == "__main__":
   main()
